From 9ee4dd427389a0edeb02dd29ed418f1856ba30fd Mon Sep 17 00:00:00 2001
From: ricolin <rlin@vexxhost.com>
Date: Mon, 24 Mar 2025 17:08:56 +0800
Subject: [PATCH 8/9] Add sync floating IP support

This patch add sync for FIP
Also correct hm_sync for FIP

Closes-Bug: #2045415

Co-authored-by: Fernando Royo <froyo@redhat.com>
Co-authored-by: Rico Lin <ricolin@ricolky.com>

Change-Id: I6dd6812e78b6ff42a27c6d1bf004250810e8b9ab
---
 ovn_octavia_provider/common/constants.py |   2 +
 ovn_octavia_provider/driver.py           |  55 ++++++++++
 ovn_octavia_provider/helper.py           | 128 +++++++++++++++++------
 3 files changed, 152 insertions(+), 33 deletions(-)

diff --git a/ovn_octavia_provider/common/constants.py b/ovn_octavia_provider/common/constants.py
index 1b7093f..7789ecb 100644
--- a/ovn_octavia_provider/common/constants.py
+++ b/ovn_octavia_provider/common/constants.py
@@ -80,7 +80,9 @@ REQ_TYPE_EXIT = 'exit'
 
 # Request information constants
 REQ_INFO_ACTION_ASSOCIATE = 'associate'
+REQ_INFO_ACTION_SYNC = 'sync'
 REQ_INFO_ACTION_DISASSOCIATE = 'disassociate'
+
 REQ_INFO_MEMBER_ADDED = 'member_added'
 REQ_INFO_MEMBER_DELETED = 'member_deleted'
 
diff --git a/ovn_octavia_provider/driver.py b/ovn_octavia_provider/driver.py
index 3c0c603..f5cdd9d 100644
--- a/ovn_octavia_provider/driver.py
+++ b/ovn_octavia_provider/driver.py
@@ -746,6 +746,60 @@ class OvnProviderDriver(driver_base.ProviderDriver):
                     ovn_lb)
                 self._ovn_helper._update_status_to_octavia(status)
 
+
+    def _fip_sync(self, loadbalancer):
+        LOG.info("Starting sync floating IP for loadbalancer "
+                 f"{loadbalancer.loadbalancer_id}")
+        if not loadbalancer.vip_port_id or not loadbalancer.vip_network_id:
+            LOG.debug("VIP Port or Network not set for loadbalancer "
+                      f"{loadbalancer.loadbalancer_id}, skip FIP sync.")
+            return
+
+        # Try to get FIP from neutron
+        fips = self._ovn_helper.get_fip_from_vip(loadbalancer)
+
+        # get FIP from LSP
+        vip_lsp = self._ovn_helper.get_lsp(
+            port_id=loadbalancer.vip_port_id,
+            network_id=loadbalancer.vip_network_id)
+        lsp_fip = vip_lsp.external_ids.get(
+            ovn_const.OVN_PORT_FIP_EXT_ID_KEY) if vip_lsp else None
+
+        if fips:
+            neutron_fip = fips[0].floating_ip_address
+            if not vip_lsp:
+                LOG.warn(
+                    "Logic Switch Port not found for port "
+                    f"{loadbalancer.vip_port_id}. "
+                    "Skip sync FIP for loadbalancer "
+                    f"{loadbalancer.loadbalancer_id}. Please "
+                    "run command `neutron-ovn-db-sync-util` "
+                    "first to sync OVN DB with Neutron DB.")
+                return
+            if lsp_fip != neutron_fip:
+                LOG.warn(
+                    "Floating IP not consistent between Logic Switch "
+                    f"Port and Neutron. Found FIP {lsp_fip} "
+                    f"in LSP {vip_lsp.name}, but we have {neutron_fip} from "
+                    "Neutron. Skip sync FIP for "
+                    f"loadbalancer {loadbalancer.loadbalancer_id}. "
+                    "Please run command `neutron-ovn-db-sync-util` "
+                    "first to sync OVN DB with Neutron DB.")
+                return
+            self._ovn_helper.vip_port_update_handler(
+                vip_lp=vip_lsp, fip=lsp_fip,
+                action=ovn_const.REQ_INFO_ACTION_SYNC)
+        else:
+            LOG.warn("Floating IP not found for loadbalancer "
+                     f"{loadbalancer.loadbalancer_id}")
+            if lsp_fip:
+                LOG.warn(
+                    "Floating IP not consistent between Logic Switch "
+                    f"Port and Neutron. Found FIP {lsp_fip} configured "
+                    f"in LSP {vip_lsp.name}, but no FIP configured from "
+                    "Neutron. Please run command `neutron-ovn-db-sync-util` "
+                    "first to sync OVN DB with Neutron DB.")
+
     def do_sync(self, **lb_filters):
         LOG.info(f"Starting sync OVN DB with Loadbalancer filter {lb_filters}")
         # TODO(froyo): get LBs from Octavia DB through openstack sdk client and
@@ -790,3 +844,4 @@ class OvnProviderDriver(driver_base.ProviderDriver):
                 provider_pools if provider_pools else o_datamodels.Unset
             )
             self._ensure_loadbalancer(provider_lb)
+            self._fip_sync(provider_lb)
diff --git a/ovn_octavia_provider/helper.py b/ovn_octavia_provider/helper.py
index 3b702a1..8555507 100644
--- a/ovn_octavia_provider/helper.py
+++ b/ovn_octavia_provider/helper.py
@@ -345,8 +345,11 @@ class OvnProviderHelper():
             request_info = {'ovn_lb': lb,
                             'vip_fip': fip,
                             'action': action}
-            self.add_request({'type': ovn_const.REQ_TYPE_HANDLE_VIP_FIP,
-                              'info': request_info})
+            if action != ovn_const.REQ_INFO_ACTION_SYNC:
+                self.add_request({'type': ovn_const.REQ_TYPE_HANDLE_VIP_FIP,
+                                  'info': request_info})
+            else:
+                self.handle_vip_fip(request_info)
 
     def _find_lb_in_ls(self, network):
         """Find LB associated to a Network using Network information
@@ -2777,34 +2780,59 @@ class OvnProviderHelper():
         external_ids = copy.deepcopy(ovn_lb.external_ids)
         commands = []
 
-        if fip_info['action'] == ovn_const.REQ_INFO_ACTION_ASSOCIATE:
+        need_ext_set = True
+        need_hc_set = True
+
+        if fip_info['action'] in (
+            ovn_const.REQ_INFO_ACTION_ASSOCIATE,
+            ovn_const.REQ_INFO_ACTION_SYNC
+        ):
             external_ids[ovn_const.LB_EXT_IDS_VIP_FIP_KEY] = (
                 fip_info['vip_fip'])
             vip_fip_info = {
                 ovn_const.LB_EXT_IDS_VIP_FIP_KEY: fip_info['vip_fip']}
-            commands.append(
-                self.ovn_nbdb_api.db_set('Load_Balancer', ovn_lb.uuid,
-                                         ('external_ids', vip_fip_info)))
-            for lb_hc in ovn_lb.health_check:
-                vip = fip_info['vip_fip']
-                lb_hc_external_ids = copy.deepcopy(lb_hc.external_ids)
-                lb_hc_external_ids[ovn_const.LB_EXT_IDS_HM_VIP] = vip
-                if self._check_lbhc_vip_format(lb_hc.vip):
-                    port = lb_hc.vip.rsplit(':')[-1]
-                    vip += ':' + port
-                else:
-                    vip = ''
-                kwargs = {
-                    'vip': vip,
-                    'options': lb_hc.options,
-                    'external_ids': lb_hc_external_ids}
-                with self.ovn_nbdb_api.transaction(check_error=True) as txn:
-                    fip_lbhc = txn.add(self.ovn_nbdb_api.db_create(
-                        'Load_Balancer_Health_Check', **kwargs))
-                    txn.add(self.ovn_nbdb_api.db_add(
-                        'Load_Balancer', ovn_lb.uuid,
-                        'health_check', fip_lbhc))
+            if fip_info['action'] == ovn_const.REQ_INFO_ACTION_SYNC:
+                # Don't need to trigger OVN DB set if external_ids not changed
+                need_ext_set = not all(
+                    ovn_lb.external_ids.get(k) ==
+                    v for k, v in vip_fip_info.items()
+                )
+                # For sync scenario, check if FIP VIP already in health_check
+                for lb_hc in ovn_lb.health_check:
+                    # All lbhc in health_check are already checked
+                    # at this stage of sync workflow in hm_purge.
+                    # So we should be able to just check health_check.
+                    if self._get_vip_lbhc(lb_hc) == fip_info['vip_fip']:
+                        need_hc_set = False
+                        break
+            if need_ext_set:
+                commands.append(
+                    self.ovn_nbdb_api.db_set('Load_Balancer', ovn_lb.uuid,
+                                             ('external_ids', vip_fip_info)))
+
+            if need_hc_set:
+                for lb_hc in ovn_lb.health_check:
+                    vip = fip_info['vip_fip']
+                    lb_hc_external_ids = copy.deepcopy(lb_hc.external_ids)
+                    lb_hc_external_ids[ovn_const.LB_EXT_IDS_HM_VIP] = vip
+                    if self._check_lbhc_vip_format(lb_hc.vip):
+                        port = lb_hc.vip.rsplit(':')[-1]
+                        vip += ':' + port
+                    else:
+                        vip = ''
+                    kwargs = {
+                        'vip': vip,
+                        'options': lb_hc.options,
+                        'external_ids': lb_hc_external_ids}
+                    with self.ovn_nbdb_api.transaction(check_error=True
+                                                       ) as txn:
+                        fip_lbhc = txn.add(self.ovn_nbdb_api.db_create(
+                            'Load_Balancer_Health_Check', **kwargs))
+                        txn.add(self.ovn_nbdb_api.db_add(
+                            'Load_Balancer', ovn_lb.uuid,
+                            'health_check', fip_lbhc))
         else:
+            # For disassociate case
             external_ids.pop(ovn_const.LB_EXT_IDS_VIP_FIP_KEY)
             commands.append(
                 self.ovn_nbdb_api.db_remove(
@@ -2821,8 +2849,14 @@ class OvnProviderHelper():
                     commands.append(self.ovn_nbdb_api.db_destroy(
                         'Load_Balancer_Health_Check', lbhc.uuid))
                     break
-
-        commands.extend(self._refresh_lb_vips(ovn_lb, external_ids))
+        commands.extend(
+            self._refresh_lb_vips(
+                ovn_lb,
+                external_ids,
+                is_sync=(
+                    fip_info['action'] == ovn_const.REQ_INFO_ACTION_SYNC)
+            )
+        )
         self._execute_commands(commands)
 
     def handle_member_dvr(self, info):
@@ -2908,6 +2942,18 @@ class OvnProviderHelper():
                             {'member': info['id'],
                              'fip': fip.external_ip})
 
+    def get_lsp(self, port_id, network_id):
+        ls_name = utils.ovn_name(network_id)
+        try:
+            ls = self.ovn_nbdb_api.lookup('Logical_Switch', ls_name)
+        except idlutils.RowNotFound:
+            LOG.warn(f"Logical Switch {ls_name} not found.")
+            return
+        for port in ls.ports:
+            if port_id in port.name:
+                # We found particular port
+                return port
+
     def _get_member_lsp(self, member_ip, member_subnet_id):
         neutron_client = clients.get_neutron_client()
         try:
@@ -2928,6 +2974,15 @@ class OvnProviderHelper():
                 # We found particular port
                 return port
 
+    def get_fip_from_vip(self, lb):
+        neutron_client = clients.get_neutron_client()
+        try:
+            return list(neutron_client.ips(port_id=lb.vip_port_id))
+        except openstack.exceptions.HttpException as e:
+            LOG.warn("Error on fetch fip for "
+                     f"{lb.loadbalancer_id} "
+                     f"Error: {str(e)}")
+
     def _add_lbhc(self, ovn_lb, pool_key, info):
         hm_id = info[constants.ID]
         status = {constants.ID: hm_id,
@@ -3346,7 +3401,7 @@ class OvnProviderHelper():
         raise idlutils.RowNotFound(table='Load_Balancer_Health_Check',
                                    col='external_ids', match=hm_id)
 
-    def _find_ovn_lb_from_hm_id(self, hm_id):
+    def _find_ovn_lb_from_hm_id(self, hm_id, lbhc_vip=None):
         lbs = self.ovn_nbdb_api.db_list_rows(
             'Load_Balancer').execute(check_error=True)
         ovn_lb = None
@@ -3357,7 +3412,14 @@ class OvnProviderHelper():
                 break
 
         try:
-            lbhcs = self._lookup_lbhcs_by_hm_id(hm_id)
+            lbhcs_by_hm_id = self._lookup_lbhcs_by_hm_id(hm_id)
+            if lbhc_vip:
+                lbhcs = []
+                for lbhc in lbhcs_by_hm_id:
+                    if lbhc.vip == lbhc_vip:
+                        lbhcs.append(lbhc)
+            else:
+                lbhcs = lbhcs_by_hm_id
         except idlutils.RowNotFound:
             LOG.debug("Loadbalancer health check %s not found!", hm_id)
             return [], ovn_lb
@@ -3872,17 +3934,17 @@ class OvnProviderHelper():
                     continue
                 fetch_hc_ids.extend([str(lbhc.uuid) for lbhc in lbhcs])
 
-            for hc_id in ovn_lb.health_check:
-                if str(hc_id.uuid) not in fetch_hc_ids:
+            for lbhc in ovn_lb.health_check:
+                if str(lbhc.uuid) not in fetch_hc_ids:
                     commands = []
                     commands.append(
                         self.ovn_nbdb_api.db_remove(
                             'Load_Balancer', ovn_lb.uuid,
-                            'health_check', hc_id.uuid))
+                            'health_check', lbhc.uuid))
                     commands.append(
                         self.ovn_nbdb_api.db_destroy(
                             'Load_Balancer_Health_Check',
-                            hc_id.uuid))
+                            lbhc.uuid))
                     try:
                         self._execute_commands(commands)
                     except idlutils.RowNotFound:
-- 
2.25.1

